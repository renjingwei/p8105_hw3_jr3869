---
title: "P8105_hw3"
author: "Jingwei Ren"
date: "10/12/2018"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

##problem1
First, do some data cleaning:

format the data to use appropriate variable names;
focus on the “Overall Health” topic
include only responses from “Excellent” to “Poor”
organize responses as a factor taking levels ordered from “Excellent” to “Poor”

```{r, clean_up}
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health" & (response == "Excellent" | response == "Very good" |
                                       response == "Good" | response == "Fair" | 
                                       response == "Poor")) %>% 
  mutate(response = factor(response, level = c("Excellent","Very good", "Good", "Fair", "Poor")))
```

Using this dataset, do or answer the following (commenting on the results of each):

In 2002, which states were observed at 7 locations?
```{r, filter}
brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n_location = n_distinct(locationdesc)) %>% 
  filter(n_location == 7)
```
CT,FL and NC were observed at 7 locations.

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r, spaghetti_plot}
location_number = brfss %>% 
  group_by(year, locationabbr) %>% 
  summarize(n_location = n_distinct(locationdesc)) 

ggplot(location_number, aes(x = year, y = n_location, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Number of Locations In Each State",
    x = "year",
    y = "number of locations",
    caption = "Data from the p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) 
```

The spaghetti plot shows the number of locations in each state from 2002 to 2010. Most states have a constant number of locations from 2002 to 2010. The number of locations has a obvious change in FL from 2006 to 2010. It first increased dramatically and then deceased sharply and finally increased back.

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r, table}
brfss %>% 
  filter(year %in% c(2002, 2006, 2010) & 
           response == "Excellent" &
           locationabbr == "NY") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = TRUE), sd_excellent = sd(data_value)) %>% 
  knitr::kable(digits = 2)

```

The table shows the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State in 2002, 2006, and 2010. The mean of proportion of excellent responses is the highest in 2002 and lowest in 2006. 2002 also has the highest standard deviation of proportion of excellent responses, while 2010 has the lowest.


For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r, proportion}
average_prop = brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(average = mean(data_value, na.rm = TRUE)) 

average_prop %>%
  ggplot(aes(x = year, y = average,color = locationabbr)) +
  geom_line() +
  labs(
    title = "Distribution of State-Level Response Averages Over Time",
    x = "year",
    y = "average proportion of each response",
    caption = "Data from the p8105.datasets package"
  ) +
  facet_wrap(~ response) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) 
```

The five-panel plot shows the distribution of these state-level averages over time. Very good has the highest proportion and poor has the lowest propotion. In each section, the proportion is fairly constant from 2002 to 2010 for almost all states, except that some states had relatively fluctuations in this period.

## problem 2

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.

```{r, description}
data("instacart")

instacart %>%
  group_by(order_id) %>%
summarize(n = n())

instacart %>%
  group_by(product_id) %>%
summarize(n = n())

instacart %>% 
  group_by(order_id) %>% 
  summarize( n_product = n_distinct(product_id))

instacart %>%
  group_by(user_id) %>%
summarize(n = n())


```

This dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns. 
Order_id provides the ID of different orders, and there are total 131209 orders.Product_id provides the ID of different products, and there are total 39123 different products. Add_to_cart_order shows order in which each product was added to cart.Reordered means that 1 if this prodcut has been ordered by this user in the past, 0 otherwise. User_id shows customer identifier.There are total 131209 users, which is same as the amount of order numbers. Order_dow: provides the day of the week on which the order was placed. Order_hour_of_day: shows the hour of the day on which the order was placed. Days_since_prior_order shows days since the last order, capped at 30, NA if order_number=1.Aisle_id provides aisle identifier and department_id provides department identifier.

examples of observations
`r head(instacart, 1) %>% knitr::kable()`

This user with the id:112108 booked an order with the id :1 at 10am on Thursday. This product is Bulgarian Yogurt. Bulgarian Yogurt has been ordered by this user in the past and this is his 4th time order this.  Bulgarian Yogurt has aisle id :120, department id: 16. It belongs to yogurt aisle and dairy eggs department. 


#### do or answer the following (commenting on the results of each):

How many aisles are there, and which aisles are the most items ordered from?
```{r, aisles}
instacart %>%
  group_by(aisle) %>%
summarize(n = n()) %>%
arrange(desc(n))


```
There are total 134 different kinds of aisles. Among them, fresh vegetables has the most items.

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r, aisle_plot}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  mutate(aisle = reorder(aisle, desc(n))) %>% 
  ggplot(
       aes(x = aisle, y = n)) +
  geom_point() +
  labs(
    title = "Number of items ordered in each aisle",
    x = "aisle",
    y = "number of items",
    caption = "Data from p8105.datasets package"
  )  +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>%
  mutate(aisle = reorder(aisle, desc(n))) %>% 
  filter(min_rank(aisle) < 20) %>% 
   ggplot(
       aes(x = aisle, y = n)) +
  geom_point() +
  labs(
    title = "Number of items ordered in each aisle Top 20",
    x = "aisle",
    y = "number of items",
    caption = "Data from p8105.datasets package"
  )  +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

top 20 number of items ordered in each aisle is ploted. Fresh vegetables has the highest number of orders, then its fresh fruits, packages vegetables fruits and yogurt.

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r aisles_table}
instacart %>% 
  filter(aisle %in% c("baking ingredients",
                      "dog food care", 
                      "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  mutate(n_ranking = min_rank(desc(n))) %>% 
  filter(n_ranking < 2) %>% 
  knitr::kable()
```

The most populay item in baking ingredients is light brown sugar with number of 499. The most populat item in dog food care is Snack Sticks Chicken & Rice Recipe Dog Treats with the number of 30. The most populay item in packaged vegetables fruits is Organic Baby Spinach with the number of 9784.

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r, lady_apple_coffe_ice_cream}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_day = ordered(order_dow, levels = c(0:6), labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  group_by(product_name, order_day) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day))) %>% 
  spread(key = order_day, value = mean_hour) %>% 
  knitr::kable()

```

The table shows that customers generally buy coffee ice cream later than pink lady apples. People buy coffee ice cream mostly after 2 pm except Friday, while people by pink lady apples mostly around noon except wednesday.


## problem 3
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.

```{r, noaa}
data("ny_noaa")

ny_noaa %>%
  group_by(id) %>%
summarize(n = n())

ny_noaa %>%
  group_by(date) %>%
summarize(n = n())


ny_noaa %>%
  group_by(id, date) %>%
summarize(mean_depth = mean(snwd),
          mean_prcp = mean (prcp))

date_range = ny_noaa %>% 
  arrange(date) 
head(date_range)
tail(date_range)
```
This dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 
Key variables: id shows Weather station ID and there are total 747 different stations. Date shows Date of observation and total 10957 days are recorded from 1981-01-01 to 2010-12-31prcp means Precipitation (tenths of mm).snow is Snowfall (mm).snwd is Snow depth (mm) tmax gives Maximum temperature (tenths of degrees C). tmin gives Minimum temperature (tenths of degrees C). There are `r sum(is.na(ny_noaa$prcp))` missing values for the precipitation colummn and `r sum(is.na(ny_noaa$tmax))` missing values for tmax and `r sum(is.na(ny_noaa$tmin))` missing values for tmin. `r sum(is.na(ny_noaa$snow))` missing values for Snowfall (mm) and `r sum(is.na(ny_noaa$snwd))` missing values for  Snow depth (mm). A large extent of the data are missing and this might be an issue in the analysis of this dataset.


####do or answer the following (commenting on the results of each):

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r clean_ny_noaa}
clean_ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp / 10, tmax = as.numeric(tmax) / 10, tmin = as.numeric(tmin) / 10)

clean_ny_noaa %>% 
  group_by(snow) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```

observations for temperature, precipitation, and snowfall are given in reasonable units. Temperature converted from tenths of degrees C to degrees C and precipitation converted from tenths of mm to mm.Temperature tmin and tmax are converted into nuermic variables. 
commonly observed values for snowfall is 0, because the chance of snowing is rare. 

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r, two-panel-plot}
 clean_ny_noaa %>% 
  filter(month %in% c("01","07")) %>% 
  mutate(month = recode(month, "01" = "January", "07" = "July")) %>% 
  group_by(year, month, id) %>% 
  summarize(average_max = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = average_max, fill = month)) +
  geom_boxplot() +
scale_x_discrete(breaks = c(1981, 1987, 1993, 1999, 2005, 2010))+
  labs(
    title = "average max temperature in January and in July in each station across years",
    x = "year",
    y = "average max temperature (C)",
    caption = "Data from p8105.datasets package"
  )  + 
  facet_grid(~ month)+
viridis::scale_color_viridis(
    name = "month", 
    discrete = TRUE)

```

Based on the two-panel plot, it is very obvious that the mean value of total average max tempertuare between Jan and July is different. The max tempertuare for Jan is lower than the max temperature for July from 1981 to 2010. Greater variability happened in Jan. Most years have outliers. For example, in 1988 July, there is a super low max temperature. All outliers in July are below the min of the max tempature, while for Jan, relatively more outliers are higher than the average. 

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.



