P8105\_hw3
================
Jingwei Ren
10/12/2018

problem1
--------

First, do some data cleaning:

format the data to use appropriate variable names; focus on the “Overall Health” topic include only responses from “Excellent” to “Poor” organize responses as a factor taking levels ordered from “Excellent” to “Poor”

``` r
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health" & (response == "Excellent" | response == "Very good" |
                                       response == "Good" | response == "Fair" | 
                                       response == "Poor")) %>% 
  mutate(response = factor(response, level = c("Excellent","Very good", "Good", "Fair", "Poor")))
```

Using this dataset, do or answer the following (commenting on the results of each):

In 2002, which states were observed at 7 locations?

``` r
brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n_location = n_distinct(locationdesc)) %>% 
  filter(n_location == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr n_location
    ##   <chr>             <int>
    ## 1 CT                    7
    ## 2 FL                    7
    ## 3 NC                    7

CT,FL and NC were observed at 7 locations.

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

``` r
location_number = brfss %>% 
  group_by(year, locationabbr) %>% 
  summarize(n_location = n_distinct(locationdesc)) 

ggplot(location_number, aes(x = year, y = n_location, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Number of Locations In Each State",
    x = "year",
    y = "number of locations",
    caption = "Data from the p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) 
```

![](p8105_hw3_files/figure-markdown_github/spaghetti_plot-1.png)

The spaghetti plot shows the number of locations in each state from 2002 to 2010. Most states have a constant number of locations from 2002 to 2010. The number of locations has a obvious change in FL from 2006 to 2010. It first increased dramatically and then deceased sharply and finally increased back.

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
brfss %>% 
  filter(year %in% c(2002, 2006, 2010) & 
           response == "Excellent" &
           locationabbr == "NY") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = TRUE), sd_excellent = sd(data_value)) %>% 
  knitr::kable(digits = 2)
```

|  year|  mean\_excellent|  sd\_excellent|
|-----:|----------------:|--------------:|
|  2002|            24.04|           4.49|
|  2006|            22.53|           4.00|
|  2010|            22.70|           3.57|

The table shows the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State in 2002, 2006, and 2010. The mean of proportion of excellent responses is the highest in 2002 and lowest in 2006. 2002 also has the highest standard deviation of proportion of excellent responses, while 2010 has the lowest.

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

``` r
average_prop = brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(average = mean(data_value, na.rm = TRUE)) 

average_prop %>%
  ggplot(aes(x = year, y = average,color = locationabbr)) +
  geom_line() +
  labs(
    title = "Distribution of State-Level Response Averages Over Time",
    x = "year",
    y = "average proportion of each response",
    caption = "Data from the p8105.datasets package"
  ) +
  facet_wrap(~ response) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) 
```

![](p8105_hw3_files/figure-markdown_github/proportion-1.png)

The five-panel plot shows the distribution of these state-level averages over time. Very good has the highest proportion and poor has the lowest propotion. In each section, the proportion is fairly constant from 2002 to 2010 for almost all states, except that some states had relatively fluctuations in this period.

problem 2
---------

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.

``` r
data("instacart")

instacart %>%
  group_by(order_id) %>%
summarize(n = n())
```

    ## # A tibble: 131,209 x 2
    ##    order_id     n
    ##       <int> <int>
    ##  1        1     8
    ##  2       36     8
    ##  3       38     9
    ##  4       96     7
    ##  5       98    49
    ##  6      112    11
    ##  7      170    17
    ##  8      218     5
    ##  9      226    13
    ## 10      349    11
    ## # ... with 131,199 more rows

``` r
instacart %>%
  group_by(product_id) %>%
summarize(n = n())
```

    ## # A tibble: 39,123 x 2
    ##    product_id     n
    ##         <int> <int>
    ##  1          1    76
    ##  2          2     4
    ##  3          3     6
    ##  4          4    22
    ##  5          5     1
    ##  6          7     1
    ##  7          8    13
    ##  8          9     5
    ##  9         10   119
    ## 10         11     2
    ## # ... with 39,113 more rows

``` r
instacart %>% 
  group_by(order_id) %>% 
  summarize( n_product = n_distinct(product_id))
```

    ## # A tibble: 131,209 x 2
    ##    order_id n_product
    ##       <int>     <int>
    ##  1        1         8
    ##  2       36         8
    ##  3       38         9
    ##  4       96         7
    ##  5       98        49
    ##  6      112        11
    ##  7      170        17
    ##  8      218         5
    ##  9      226        13
    ## 10      349        11
    ## # ... with 131,199 more rows

``` r
instacart %>%
  group_by(user_id) %>%
summarize(n = n())
```

    ## # A tibble: 131,209 x 2
    ##    user_id     n
    ##      <int> <int>
    ##  1       1    11
    ##  2       2    31
    ##  3       5     9
    ##  4       7     9
    ##  5       8    18
    ##  6       9    22
    ##  7      10     4
    ##  8      13     5
    ##  9      14    11
    ## 10      17     6
    ## # ... with 131,199 more rows

This dataset has 1384617 rows and 15 columns. Order\_id provides the ID of different orders, and there are total 131209 orders.Product\_id provides the ID of different products, and there are total 39123 different products. Add\_to\_cart\_order shows order in which each product was added to cart.Reordered means that 1 if this prodcut has been ordered by this user in the past, 0 otherwise. User\_id shows customer identifier.There are total 131209 users, which is same as the amount of order numbers. Order\_dow: provides the day of the week on which the order was placed. Order\_hour\_of\_day: shows the hour of the day on which the order was placed. Days\_since\_prior\_order shows days since the last order, capped at 30, NA if order\_number=1.Aisle\_id provides aisle identifier and department\_id provides department identifier.

examples of observations

|  order\_id|  product\_id|  add\_to\_cart\_order|  reordered|  user\_id| eval\_set |  order\_number|  order\_dow|  order\_hour\_of\_day|  days\_since\_prior\_order| product\_name    |  aisle\_id|  department\_id| aisle  | department |
|----------:|------------:|---------------------:|----------:|---------:|:----------|--------------:|-----------:|---------------------:|--------------------------:|:-----------------|----------:|---------------:|:-------|:-----------|
|          1|        49302|                     1|          1|    112108| train     |              4|           4|                    10|                          9| Bulgarian Yogurt |        120|              16| yogurt | dairy eggs |

This user with the id:112108 booked an order with the id :1 at 10am on Thursday. This product is Bulgarian Yogurt. Bulgarian Yogurt has been ordered by this user in the past and this is his 4th time order this. Bulgarian Yogurt has aisle id :120, department id: 16. It belongs to yogurt aisle and dairy eggs department.

#### do or answer the following (commenting on the results of each):

How many aisles are there, and which aisles are the most items ordered from?

``` r
instacart %>%
  group_by(aisle) %>%
summarize(n = n()) %>%
arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

There are total 134 different kinds of aisles. Among them, fresh vegetables has the most items.

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  mutate(aisle = reorder(aisle, desc(n))) %>% 
  ggplot(
       aes(x = aisle, y = n)) +
  geom_point() +
  labs(
    title = "Number of items ordered in each aisle",
    x = "aisle",
    y = "number of items",
    caption = "Data from p8105.datasets package"
  )  +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

![](p8105_hw3_files/figure-markdown_github/aisle_plot-1.png)

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>%
  mutate(aisle = reorder(aisle, desc(n))) %>% 
  filter(min_rank(aisle) < 20) %>% 
   ggplot(
       aes(x = aisle, y = n)) +
  geom_point() +
  labs(
    title = "Number of items ordered in each aisle Top 20",
    x = "aisle",
    y = "number of items",
    caption = "Data from p8105.datasets package"
  )  +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

![](p8105_hw3_files/figure-markdown_github/aisle_plot-2.png)

top 20 number of items ordered in each aisle is ploted. Fresh vegetables has the highest number of orders, then its fresh fruits, packages vegetables fruits and yogurt.

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients",
                      "dog food care", 
                      "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  mutate(n_ranking = min_rank(desc(n))) %>% 
  filter(n_ranking < 2) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |     n|  n\_ranking|
|:---------------------------|:----------------------------------------------|-----:|-----------:|
| baking ingredients         | Light Brown Sugar                             |   499|           1|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30|           1|
| packaged vegetables fruits | Organic Baby Spinach                          |  9784|           1|

The most populay item in baking ingredients is light brown sugar with number of 499. The most populat item in dog food care is Snack Sticks Chicken & Rice Recipe Dog Treats with the number of 30. The most populay item in packaged vegetables fruits is Organic Baby Spinach with the number of 9784.

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_day = ordered(order_dow, levels = c(0:6), labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  group_by(product_name, order_day) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day))) %>% 
  spread(key = order_day, value = mean_hour) %>% 
  knitr::kable()
```

| product\_name    |  Sunday|  Monday|  Tuesday|  Wednesday|  Thursday|  Friday|  Saturday|
|:-----------------|-------:|-------:|--------:|----------:|---------:|-------:|---------:|
| Coffee Ice Cream |      14|      14|       15|         15|        15|      12|        14|
| Pink Lady Apples |      13|      11|       12|         14|        12|      13|        12|

The table shows that customers generally buy coffee ice cream later than pink lady apples. People buy coffee ice cream mostly after 2 pm except Friday, while people by pink lady apples mostly around noon except wednesday.

problem 3
---------

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.

``` r
data("ny_noaa")

ny_noaa %>%
  group_by(id) %>%
summarize(n = n())
```

    ## # A tibble: 747 x 2
    ##    id              n
    ##    <chr>       <int>
    ##  1 US1NYAB0001  1157
    ##  2 US1NYAB0006   852
    ##  3 US1NYAB0010   822
    ##  4 US1NYAB0016   214
    ##  5 US1NYAB0017   459
    ##  6 US1NYAB0021   365
    ##  7 US1NYAB0022   273
    ##  8 US1NYAB0023   365
    ##  9 US1NYAB0025   215
    ## 10 US1NYAL0002   549
    ## # ... with 737 more rows

``` r
ny_noaa %>%
  group_by(date) %>%
summarize(n = n())
```

    ## # A tibble: 10,957 x 2
    ##    date           n
    ##    <date>     <int>
    ##  1 1981-01-01   236
    ##  2 1981-01-02   236
    ##  3 1981-01-03   236
    ##  4 1981-01-04   236
    ##  5 1981-01-05   236
    ##  6 1981-01-06   236
    ##  7 1981-01-07   236
    ##  8 1981-01-08   236
    ##  9 1981-01-09   236
    ## 10 1981-01-10   236
    ## # ... with 10,947 more rows

``` r
ny_noaa %>%
  group_by(id, date) %>%
summarize(mean_depth = mean(snwd),
          mean_prcp = mean (prcp))
```

    ## # A tibble: 2,595,176 x 4
    ## # Groups:   id [?]
    ##    id          date       mean_depth mean_prcp
    ##    <chr>       <date>          <dbl>     <dbl>
    ##  1 US1NYAB0001 2007-11-01         NA        NA
    ##  2 US1NYAB0001 2007-11-02         NA        NA
    ##  3 US1NYAB0001 2007-11-03         NA        NA
    ##  4 US1NYAB0001 2007-11-04         NA        NA
    ##  5 US1NYAB0001 2007-11-05         NA        NA
    ##  6 US1NYAB0001 2007-11-06         NA        NA
    ##  7 US1NYAB0001 2007-11-07         NA        NA
    ##  8 US1NYAB0001 2007-11-08         NA        NA
    ##  9 US1NYAB0001 2007-11-09         NA        NA
    ## 10 US1NYAB0001 2007-11-10         NA        NA
    ## # ... with 2,595,166 more rows

``` r
date_range = ny_noaa %>% 
  arrange(date) 
head(date_range)
```

    ## # A tibble: 6 x 7
    ##   id          date        prcp  snow  snwd tmax  tmin 
    ##   <chr>       <date>     <int> <int> <int> <chr> <chr>
    ## 1 USC00300023 1981-01-01     0     0     0 -56   <NA> 
    ## 2 USC00300055 1981-01-01     0     0    76 -50   -128 
    ## 3 USC00300063 1981-01-01     0    NA    NA -83   -150 
    ## 4 USC00300085 1981-01-01     3     3   102 -44   -139 
    ## 5 USC00300093 1981-01-01     0    NA   102 -39   -128 
    ## 6 USC00300183 1981-01-01     0     0    76 -33   -117

``` r
tail(date_range)
```

    ## # A tibble: 6 x 7
    ##   id          date        prcp  snow  snwd tmax  tmin 
    ##   <chr>       <date>     <int> <int> <int> <chr> <chr>
    ## 1 USW00094725 2010-12-31     0    NA    NA 106   6    
    ## 2 USW00094728 2010-12-31     0     0   330 72    22   
    ## 3 USW00094740 2010-12-31     0    NA    NA 89    -10  
    ## 4 USW00094745 2010-12-31     0    NA    NA 78    -38  
    ## 5 USW00094789 2010-12-31     0     0   229 44    -11  
    ## 6 USW00094790 2010-12-31     0    NA    NA 117   44

This dataset has 2595176 rows and 7 columns. Key variables: id shows Weather station ID and there are total 747 different stations. Date shows Date of observation and total 10957 days are recorded from 1981-01-01 to 2010-12-31prcp means Precipitation (tenths of mm).snow is Snowfall (mm).snwd is Snow depth (mm) tmax gives Maximum temperature (tenths of degrees C). tmin gives Minimum temperature (tenths of degrees C). There are 145838 missing values for the precipitation colummn and 1134358 missing values for tmax and 1134420 missing values for tmin. 381221 missing values for Snowfall (mm) and 591786 missing values for Snow depth (mm). A large extent of the data are missing and this might be an issue in the analysis of this dataset.

#### do or answer the following (commenting on the results of each):

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

``` r
clean_ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp / 10, tmax = as.numeric(tmax) / 10, tmin = as.numeric(tmin) / 10)

clean_ny_noaa %>% 
  group_by(snow) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```

    ## # A tibble: 282 x 2
    ##     snow       n
    ##    <int>   <int>
    ##  1     0 2008508
    ##  2    NA  381221
    ##  3    25   31022
    ##  4    13   23095
    ##  5    51   18274
    ##  6    76   10173
    ##  7     8    9962
    ##  8     5    9748
    ##  9    38    9197
    ## 10     3    8790
    ## # ... with 272 more rows

observations for temperature, precipitation, and snowfall are given in reasonable units. Temperature converted from tenths of degrees C to degrees C and precipitation converted from tenths of mm to mm.Temperature tmin and tmax are converted into nuermic variables. commonly observed values for snowfall is 0, because the chance of snowing is rare.

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

``` r
 clean_ny_noaa %>% 
  filter(month %in% c("01","07")) %>% 
  mutate(month = recode(month, "01" = "January", "07" = "July")) %>% 
  group_by(year, month, id) %>% 
  summarize(average_max = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = average_max, fill = month)) +
  geom_boxplot() +
scale_x_discrete(breaks = c(1981, 1987, 1993, 1999, 2005, 2010))+
  labs(
    title = "average max temperature in January and in July in each station across years",
    x = "year",
    y = "average max temperature (C)",
    caption = "Data from p8105.datasets package"
  )  + 
  facet_grid(~ month)+
viridis::scale_color_viridis(
    name = "month", 
    discrete = TRUE)
```

    ## Warning: Removed 5970 rows containing non-finite values (stat_boxplot).

![](p8105_hw3_files/figure-markdown_github/two-panel-plot-1.png)

Based on the two-panel plot, it is very obvious that the mean value of total average max tempertuare between Jan and July is different. The max tempertuare for Jan is lower than the max temperature for July from 1981 to 2010. Greater variability happened in Jan. Most years have outliers. For example, in 1988 July, there is a super low max temperature. All outliers in July are below the min of the max tempature, while for Jan, relatively more outliers are higher than the average.

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

``` r
library(hexbin)
temperature= clean_ny_noaa %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(
    title = " tmax vs tmin for the full dataset",
    x = "maximum temperature (C)",
    y = "minimum temperature (C)",
 caption = "Data from p8105.datasets package"
  )

snowfall=clean_ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) +
  geom_boxplot() +
  scale_x_discrete(breaks = c(1981, 1987, 1993, 1999, 2005, 2010)) +
    labs(
      x = "Year",
      y = "Snowfall (mm)",
      title = "alues greater than 0 and less than 100 separately by year",
       caption = "Data from p8105.datasets package"
    )
temperature/snowfall
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

![](p8105_hw3_files/figure-markdown_github/tmax_tmin_and_snowfall-1.png)

Generally, tmax and tmin are positively correlated. As tmax increases, tmin increases. The mostly observed tmax is between 0 to 25 C and the mostly observed tmin is between -15 to 18 C. Most years have fairly simialr and constant snowfall,except 1998,2006 and 2010. Overall, the distribution of snowfall is stable, with the median of 25.
